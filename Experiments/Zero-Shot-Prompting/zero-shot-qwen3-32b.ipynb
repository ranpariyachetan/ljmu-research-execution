{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETsj-zAOzTyq",
        "outputId": "d2cb6dd0-fa9b-4268-fe6e-6e375cbd9aa8"
      },
      "outputs": [],
      "source": [
        "!pip install groq shap lime pandas matplotlib seaborn scikit-learn transformers nltk dotenv bs4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uWjLz2IzYxc"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 1. SETUP AND IMPORTS\n",
        "# ============================================================================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import time\n",
        "from typing import Dict, List, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# LLM Integration\n",
        "from groq import Groq\n",
        "\n",
        "# NLP and Evaluation\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, classification_report\n",
        ")\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# XAI Libraries\n",
        "import shap\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download required NLTK data\n",
        "try:\n",
        "    nltk.download('vader_lexicon', quiet=True)\n",
        "except:\n",
        "    pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "if 'google.colab' in sys.modules:\n",
        "    from google.colab import userdata\n",
        "    LLM_API_KEY = userdata.get(\"GROQ_API_KEY\")\n",
        "    print(\"Running in Google Colab\")\n",
        "    IN_COLAB = True\n",
        "else:\n",
        "    print(\"Not running in Google Colab\")\n",
        "    LLM_API_KEY = os.getenv(\"GROQ_API_KEY\")\n",
        "    IN_COLAB = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mRoC0YjY0epg"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 2. OPENAI CLIENT SETUP\n",
        "# ============================================================================\n",
        "\n",
        "# Initialize OpenAI client\n",
        "MODEL_NAME = \"qwen/qwen3-32b\"\n",
        "client = Groq(api_key=LLM_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2jfnlbZh0c4W"
      },
      "outputs": [],
      "source": [
        "def query_llm(prompt: str, model: str, max_retries: int = 3) -> str:\n",
        "    \"\"\"Query gpt-oss-120b via OpenAI with retry logic\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0,  # Deterministic for consistency\n",
        "                max_tokens=500,\n",
        "                top_p=1,\n",
        "            )\n",
        "            return response.choices[0].message.content.strip()\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                time.sleep(2 ** attempt)  # Exponential backoff\n",
        "                continue\n",
        "            else:\n",
        "                print(f\"Error after {max_retries} attempts: {e}\")\n",
        "                return \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Additional imports for parallel processing\n",
        "import multiprocessing as mp\n",
        "# from concurrent.futures import ProcessPoolExecutor, as_completed\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "print(f\"Available CPU cores: {mp.cpu_count()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVsaIOhV0oPh",
        "outputId": "a27c4027-735c-4871-a5b6-f5790e61a85d"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 3. DATA LOADING AND PREPROCESSING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"DATA LOADING AND PREPROCESSING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "DATA_PATH = '../Datasets/Financial-QA-10k.csv'\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    DATA_PATH = 'Financial-QA-10k.csv'  # Colab path\n",
        "# Load dataset (replace with your actual path)\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "print(f\"\\n✓ Dataset loaded: {len(df)} records\")\n",
        "print(f\"✓ Columns: {list(df.columns)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rC3AC4nx0v0-"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Text Cleaning Function\n",
        "def clean_text(text: str) -> str:\n",
        "    \"\"\"Clean and normalize text\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    # Remove extra whitespace\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Remove special characters but keep basic punctuation\n",
        "    text = re.sub(r'[^\\w\\s.,!?-]', '', text)\n",
        "    return text.strip()\n",
        "\n",
        "def clean_html(text: str) -> str:\n",
        "    \"\"\"Remove HTML tags and entities.\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "\n",
        "    # Parse HTML and extract text\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    text = soup.get_text()\n",
        "\n",
        "    # Clean up HTML entities\n",
        "    text = re.sub(r'&[a-zA-Z]+;', ' ', text)\n",
        "    text = re.sub(r'&#[0-9]+;', ' ', text)\n",
        "\n",
        "    return text\n",
        "\n",
        "def clean_special_chars(text: str) -> str:\n",
        "    \"\"\"Remove special characters and normalize text.\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', text)\n",
        "    text = re.sub(r'www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', ' ', text)\n",
        "\n",
        "    # Remove email addresses\n",
        "    text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\\\.[a-zA-Z]{2,}', ' ', text)\n",
        "\n",
        "    # Clean excessive whitespace\n",
        "    text = re.sub(r'\\\\s+', ' ', text)\n",
        "\n",
        "    # Remove excessive punctuation\n",
        "    text = re.sub(r'[!]{2,}', '!', text)\n",
        "    text = re.sub(r'[?]{2,}', '?', text)\n",
        "    text = re.sub(r'[.]{3,}', '...', text)\n",
        "\n",
        "    return text.strip()\n",
        "\n",
        "def normalize_case(text: str) -> str:\n",
        "    \"\"\"Normalize text case while preserving proper nouns.\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    # Convert to lowercase but preserve some financial terms\n",
        "    financial_terms = ['USD', 'CEO', 'CFO', 'SEC', 'GAAP', 'EBITDA', 'ROI', 'IPO', 'NYSE', 'NASDAQ']\n",
        "\n",
        "    # Temporarily replace financial terms\n",
        "    temp_replacements = {}\n",
        "    for i, term in enumerate(financial_terms):\n",
        "        if term in text:\n",
        "            placeholder = f\"__FINANCIAL_TERM_{i}__\"\n",
        "            temp_replacements[placeholder] = term\n",
        "            text = text.replace(term, placeholder)\n",
        "\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "\n",
        "    # Restore financial terms\n",
        "    for placeholder, original in temp_replacements.items():\n",
        "        text = text.replace(placeholder, original)\n",
        "\n",
        "    return text\n",
        "\n",
        "def preprocess(text: str) -> str:\n",
        "    \"\"\"Apply all preprocessing steps.\"\"\"\n",
        "    # Step 1: Clean HTML\n",
        "    text = clean_html(text)\n",
        "\n",
        "    # Step 2: Clean special characters\n",
        "    text = clean_special_chars(text)\n",
        "\n",
        "    # Step 3: Normalize case\n",
        "    text = normalize_case(text)\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7FyHbT2ewEdR",
        "outputId": "83828e89-cc85-4492-c002-b81223b957e9"
      },
      "outputs": [],
      "source": [
        "# Apply cleaning\n",
        "df['question'] = df['question'].apply(clean_text)\n",
        "df['answer'] = df['answer'].apply(clean_text)\n",
        "df['context'] = df['context'].apply(clean_text)\n",
        "df['combined_text'] = df['question'] + ' ' + df['answer'] + ' ' + df['context']\n",
        "df['combined_text'] = df['combined_text'].apply(preprocess)\n",
        "\n",
        "print(\"✓ Text cleaning completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZR-7pX500Vx",
        "outputId": "9975d9bc-0390-46ba-b4b4-21fd408ea789"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 4. GROUND TRUTH LABEL GENERATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"GENERATING GROUND TRUTH LABELS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# (A) SENTIMENT LABELS - Derived from context\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "def get_sentiment_label(text: str) -> str:\n",
        "    \"\"\"Derive sentiment label using VADER\"\"\"\n",
        "    score = sia.polarity_scores(text)['compound']\n",
        "    if score > 0.05:\n",
        "        return 'Positive'\n",
        "    elif score < -0.05:\n",
        "        return 'Negative'\n",
        "    else:\n",
        "        return 'Neutral'\n",
        "\n",
        "df['sentiment_true'] = df['context'].apply(get_sentiment_label)\n",
        "# df['sentiment_true'] = df['context'].apply(get_sentiment_label)\n",
        "print(f\"\\n✓ Sentiment labels generated\")\n",
        "print(f\"  Distribution: {df['sentiment_true'].value_counts().to_dict()}\")\n",
        "\n",
        "# (B) TOPIC LABELS - Derived from questions\n",
        "def get_topic_label(question: str) -> str:\n",
        "    \"\"\"Extract topic from question using keyword matching\"\"\"\n",
        "    q = question.lower()\n",
        "    if any(word in q for word in ['revenue', 'sales', 'income', 'profit']):\n",
        "        return 'Revenue'\n",
        "    elif any(word in q for word in ['risk', 'threat', 'challenge', 'vulnerability']):\n",
        "        return 'Risk'\n",
        "    elif any(word in q for word in ['operation', 'efficiency', 'process', 'performance']):\n",
        "        return 'Operations'\n",
        "    elif any(word in q for word in ['legal', 'litigation', 'lawsuit', 'compliance']):\n",
        "        return 'Legal'\n",
        "    elif any(word in q for word in ['management', 'executive', 'leadership', 'board']):\n",
        "        return 'Management'\n",
        "    else:\n",
        "        return 'Other'\n",
        "\n",
        "df['topic_true'] = df['question'].apply(get_topic_label)\n",
        "print(f\"\\n✓ Topic labels generated\")\n",
        "print(f\"  Distribution: {df['topic_true'].value_counts().to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4QeC7OAa0_mh",
        "outputId": "1c7d954a-d734-4ca3-aa37-c2a2a3b1f24f"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 5. ZERO-SHOT PROMPT TEMPLATES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ZERO-SHOT PROMPT DESIGN\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def create_qa_prompt(question: str, context: str) -> str:\n",
        "    \"\"\"Zero-shot prompt for Question Answering\"\"\"\n",
        "    return f\"\"\"You are a financial analysis expert. Answer the following question based on the provided context from a 10-K filing.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Provide a concise and accurate answer based solely on the information in the context. If the answer is not in the context, say \"Cannot be determined from the context.\"\n",
        "\n",
        "Answer:\"\"\"\n",
        "\n",
        "def create_sentiment_prompt(text: str) -> str:\n",
        "    \"\"\"Zero-shot prompt for Sentiment Classification\"\"\"\n",
        "    return f\"\"\"Analyze the sentiment of the following financial text. Classify it as exactly one of: Positive, Negative, or Neutral.\n",
        "\n",
        "Text: {text}\n",
        "\n",
        "Consider financial terminology and context. Respond with only one word: Positive, Negative, or Neutral.\n",
        "\n",
        "Sentiment:\"\"\"\n",
        "\n",
        "def create_topic_prompt(question: str) -> str:\n",
        "    \"\"\"Zero-shot prompt for Topic Detection\"\"\"\n",
        "    return f\"\"\"Classify the following financial question into exactly one category from this list:\n",
        "[Revenue, Risk, Operations, Legal, Management, Other]\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Respond with only one category name from the list above.\n",
        "\n",
        "Category:\"\"\"\n",
        "\n",
        "print(\"✓ Zero-shot prompts designed for all three tasks\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "elRrxq8e1M4_",
        "outputId": "7e327584-129c-452a-dde0-24f87fa4b825"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 6. TASK 1: QUESTION ANSWERING\n",
        "# ============================================================================\n",
        "from IPython.display import clear_output\n",
        "import time\n",
        "\n",
        "def exact_match(pred: str, gold: str) -> int:\n",
        "    \"\"\"Calculate exact match score\"\"\"\n",
        "    return int(pred.strip().lower() == gold.strip().lower())\n",
        "\n",
        "def token_f1(pred: str, gold: str) -> float:\n",
        "    \"\"\"Calculate token-level F1 score\"\"\"\n",
        "    pred_tokens = set(pred.lower().split())\n",
        "    gold_tokens = set(gold.lower().split())\n",
        "\n",
        "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    overlap = len(pred_tokens & gold_tokens)\n",
        "    if overlap == 0:\n",
        "        return 0.0\n",
        "\n",
        "    precision = overlap / len(pred_tokens)\n",
        "    recall = overlap / len(gold_tokens)\n",
        "\n",
        "    return 2 * precision * recall / (precision + recall)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions\n",
        "\n",
        "def generate_qa_predictions_sequential():    \n",
        "    qa_predictions = []\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating QA Predictions\"):\n",
        "        prompt = create_qa_prompt(row['question'], row['context'])\n",
        "        pred = query_llm(prompt, MODEL_NAME)\n",
        "        qa_predictions.append(pred)\n",
        "    print(f\"\\n✓ QA predictions completed\")\n",
        "    return qa_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "def process_qa_chunk(chunk, model, api_key, chunk_id):\n",
        "    client = Groq(api_key=api_key)\n",
        "\n",
        "    qa_predictions = []\n",
        "\n",
        "    try:\n",
        "        for idx, row in tqdm(enumerate(chunk), total=len(chunk), desc=f\"Generating QA Predictions for chunk_id {chunk_id}\"):\n",
        "            prompt = create_qa_prompt(row[0], row[1])\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0,  # Deterministic for consistency\n",
        "                max_tokens=500,\n",
        "                top_p=1,\n",
        "            )\n",
        "            qa_predictions.append(response.choices[0].message.content.strip())\n",
        "            time.sleep(0.05)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in QA processing of chunk_id {chunk_id}: {e}\")\n",
        "        qa_predictions.append(\"No answer found\")\n",
        "\n",
        "    return qa_predictions\n",
        "\n",
        "def generate_qa_predictions_parallel(n_workers, inputs):\n",
        "    # Estimate processing time\n",
        "    estimated_time_sequential = len(inputs) * 5 / 60  # 5 seconds per text\n",
        "    estimated_time_parallel = estimated_time_sequential / n_workers \n",
        "    print(f\"Estimated time:\")\n",
        "    print(f\" Sequential: ~{estimated_time_sequential:.1f} minutes\")\n",
        "    print(f\" Parallel ({n_workers} workers): ~{estimated_time_parallel:.1f} minutes\")\n",
        "    print(f\" Speedup: ~{estimated_time_sequential/estimated_time_parallel:.1f}x faster!\")\n",
        "\n",
        "    print(f\"Using {n_workers} workers for parallel question answering...\")\n",
        "    print(f\"Processing {len(inputs)} texts in parallel batches...\")\n",
        "\n",
        "    # Split texts into chunks for each worker\n",
        "    chunk_size = max(1, len(inputs) // n_workers)\n",
        "    text_chunks = []\n",
        "\n",
        "    for i in range(0, len(inputs), chunk_size):\n",
        "        text_chunks.append(inputs[i:i+chunk_size])\n",
        "\n",
        "    qa_predictions = []\n",
        "\n",
        "    try:\n",
        "        with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
        "            # Submit all chunks to workers\n",
        "            future_to_chunk = {\n",
        "                executor.submit(process_qa_chunk, chunk, MODEL_NAME, LLM_API_KEY, i): (chunk, i)\n",
        "                for i, chunk in enumerate(text_chunks)\n",
        "            }\n",
        "\n",
        "            chunk_results = {}\n",
        "            with tqdm(total=len(inputs), desc=\"Processing QA Predictions\") as pbar:\n",
        "                for future in as_completed(future_to_chunk):\n",
        "                    try:\n",
        "                        chunk_data,chunk_id = future_to_chunk[future]\n",
        "                        results = future.result()\n",
        "                        chunk_results[chunk_id] = results\n",
        "                        pbar.update(len(results))\n",
        "                        print(f\"Processed chunk_id {chunk_id} with {len(results)} predictions\")\n",
        "                    except Exception as e:\n",
        "                        chunk_data, chunk_id = future_to_chunk[future]\n",
        "                        error_results = [\"No answer found\"] * len(chunk_data)\n",
        "                        chunk_results[chunk_id] = error_results\n",
        "                        print(f\"Error in QA processing of chunk_id {chunk_id}: {e}\")\n",
        "            for chunk_id in sorted(chunk_results.keys()):\n",
        "                qa_predictions.extend(chunk_results[chunk_id])\n",
        "    except Exception as e:\n",
        "        print(f\"Error in sentiment processing: {e}\")\n",
        "    return qa_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 6. TASK 1: QUESTION ANSWERING\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TASK 1: QUESTION ANSWERING\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "cpu_count = mp.cpu_count()\n",
        "\n",
        "if cpu_count > 4:\n",
        "    n_workers = min(mp.cpu_count(), 8)  # Cap at 8 workers for API rate limits\n",
        "else:\n",
        "    n_workers = cpu_count\n",
        "\n",
        "input_texts = df['question'].tolist()\n",
        "context_texts = df['context'].tolist()\n",
        "\n",
        "inputs = list(zip(input_texts, context_texts))\n",
        "\n",
        "qa_preds = []\n",
        "if n_workers < 4 or len(inputs) < 100:\n",
        "    print(\"Using sequential generation for Question Answering\")\n",
        "    qa_preds = generate_qa_predictions_sequential()\n",
        "else:\n",
        "    print(\"Using parallel generation for Question Answering\")\n",
        "    qa_preds = generate_qa_predictions_parallel(n_workers,inputs)\n",
        "\n",
        "df['qa_pred'] = qa_preds\n",
        "print(f\"\\n✓ QA predictions completed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "udLR7WVQgZwe",
        "outputId": "d42f0bff-2487-41a3-fdee-9068a3137838"
      },
      "outputs": [],
      "source": [
        "# Evaluate QA\n",
        "qa_em_scores = []\n",
        "qa_f1_scores = []\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    em = exact_match(row['qa_pred'], row['answer'])\n",
        "    f1 = token_f1(row['qa_pred'], row['answer'])\n",
        "    qa_em_scores.append(em)\n",
        "    qa_f1_scores.append(f1)\n",
        "\n",
        "# Populate data frame\n",
        "df['qa_em'] = qa_em_scores\n",
        "df['qa_f1'] = qa_f1_scores\n",
        "\n",
        "# QA Results\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"QUESTION ANSWERING RESULTS\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"Exact Match (EM): {np.mean(qa_em_scores):.4f}\")\n",
        "print(f\"Token F1 Score:   {np.mean(qa_f1_scores):.4f}\")\n",
        "\n",
        "# Display sample predictions\n",
        "print(\"\\nSample QA Predictions:\")\n",
        "print(\"-\" * 80)\n",
        "for idx in range(min(3, len(df))):\n",
        "  print(f\"\\nQuestion: {df.iloc[idx]['question']}\")\n",
        "  print(f\"True Answer: {df.iloc[idx]['answer']}\")\n",
        "  print(f\"Predicted: {df.iloc[idx]['qa_pred']}\")\n",
        "  print(f\"EM: {df.iloc[idx]['qa_em']}, F1: {df.iloc[idx]['qa_f1']:.3f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions\n",
        "\n",
        "def generate_sentiment_predictions_sequential():    \n",
        "    sentiment_predictions = []\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating Sentiment Predictions\"):\n",
        "        prompt = create_sentiment_prompt(row['context'])\n",
        "        pred = query_llm(prompt, MODEL_NAME)\n",
        "        sentiment_predictions.append(pred)\n",
        "    print(f\"\\n✓ Sentiment predictions completed\")\n",
        "    return sentiment_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "def process_sentiment_chunk(chunk, model, api_key, chunk_id):\n",
        "    client = Groq(api_key=api_key)\n",
        "\n",
        "    sentiment_predictions = []\n",
        "\n",
        "    try:\n",
        "        for idx, text in tqdm(enumerate(chunk), total=len(chunk), desc=f\"Generating Sentiment Predictions for chunk_id {chunk_id}\"):\n",
        "            prompt = create_sentiment_prompt(text)\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0,  # Deterministic for consistency\n",
        "                max_tokens=500,\n",
        "                top_p=1,\n",
        "            )\n",
        "            sentiment_predictions.append(response.choices[0].message.content.strip())\n",
        "            time.sleep(0.05)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in Sentiment processing of chunk_id {chunk_id}: {e}\")\n",
        "        sentiment_predictions.append(\"No answer found\")\n",
        "\n",
        "    return sentiment_predictions\n",
        "\n",
        "def generate_sentiment_predictions_parallel(n_workers, inputs):\n",
        "    # Estimate processing time\n",
        "    estimated_time_sequential = len(inputs) * 5 / 60  # 5 seconds per text\n",
        "    estimated_time_parallel = estimated_time_sequential / n_workers \n",
        "    print(f\"Estimated time:\")\n",
        "    print(f\" Sequential: ~{estimated_time_sequential:.1f} minutes\")\n",
        "    print(f\" Parallel ({n_workers} workers): ~{estimated_time_parallel:.1f} minutes\")\n",
        "    print(f\" Speedup: ~{estimated_time_sequential/estimated_time_parallel:.1f}x faster!\")\n",
        "\n",
        "    print(f\"Using {n_workers} workers for parallel sentiment classification...\")\n",
        "    print(f\"Processing {len(inputs)} texts in parallel batches...\")\n",
        "\n",
        "    # Split texts into chunks for each worker\n",
        "    chunk_size = max(1, len(inputs) // n_workers)\n",
        "    text_chunks = []\n",
        "\n",
        "    for i in range(0, len(inputs), chunk_size):\n",
        "        text_chunks.append(inputs[i:i+chunk_size])\n",
        "\n",
        "    sentiment_predictions = []\n",
        "\n",
        "    try:\n",
        "        with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
        "            # Submit all chunks to workers\n",
        "            future_to_chunk = {\n",
        "                executor.submit(process_sentiment_chunk, chunk, MODEL_NAME, LLM_API_KEY, i): (chunk, i)\n",
        "                for i, chunk in enumerate(text_chunks)\n",
        "            }\n",
        "\n",
        "            chunk_results = {}\n",
        "            with tqdm(total=len(inputs), desc=\"Processing Sentiment Predictions\") as pbar:\n",
        "                for future in as_completed(future_to_chunk):\n",
        "                    try:\n",
        "                        chunk_data,chunk_id = future_to_chunk[future]\n",
        "                        results = future.result()\n",
        "                        chunk_results[chunk_id] = results\n",
        "                        pbar.update(len(results))\n",
        "                        print(f\"Processed chunk_id {chunk_id} with {len(results)} predictions\")\n",
        "                    except Exception as e:\n",
        "                        chunk_data, chunk_id = future_to_chunk[future]\n",
        "                        error_results = [\"No answer found\"] * len(chunk_data)\n",
        "                        chunk_results[chunk_id] = error_results\n",
        "                        print(f\"Error in Sentiment processing of chunk_id {chunk_id}: {e}\")\n",
        "            for chunk_id in sorted(chunk_results.keys()):\n",
        "                sentiment_predictions.extend(chunk_results[chunk_id])\n",
        "    except Exception as e:\n",
        "        print(f\"Error in sentiment processing: {e}\")\n",
        "    return sentiment_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 7. TASK 2: SENTIMENT CLASSIFICATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TASK 2: SENTIMENT CLASSIFICATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Generate predictions\n",
        "print(\"\\n⏳ Generating sentiment predictions...\")\n",
        "\n",
        "cpu_count = mp.cpu_count()\n",
        "\n",
        "if cpu_count > 4:\n",
        "    n_workers = min(mp.cpu_count(), 8)  # Cap at 8 workers for API rate limits\n",
        "else:\n",
        "    n_workers = cpu_count\n",
        "\n",
        "inputs = df['context'].tolist()\n",
        "\n",
        "sentiment_preds = []\n",
        "if n_workers < 4 or len(inputs) < 100:\n",
        "    print(\"Using sequential generation for Sentiment Classification\")\n",
        "    sentiment_preds = generate_sentiment_predictions_sequential()\n",
        "else:\n",
        "    print(\"Using parallel generation for Sentiment Classification\")\n",
        "    sentiment_preds = generate_sentiment_predictions_parallel(n_workers,inputs)\n",
        "\n",
        "df['sentiment_pred'] = sentiment_preds\n",
        "print(f\"\\n✓ Sentiment predictions completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hcpHyZ4uwnzi",
        "outputId": "a38b9578-f8d2-4bb6-cd3f-1928f1ce5b58"
      },
      "outputs": [],
      "source": [
        "# Evaluate Sentiment\n",
        "y_true_sent = df['sentiment_true']\n",
        "y_pred_sent = df['sentiment_pred']\n",
        "\n",
        "sent_accuracy = accuracy_score(y_true_sent, y_pred_sent)\n",
        "sent_precision = precision_score(y_true_sent, y_pred_sent, average='macro', zero_division=0)\n",
        "sent_recall = recall_score(y_true_sent, y_pred_sent, average='macro', zero_division=0)\n",
        "sent_f1 = f1_score(y_true_sent, y_pred_sent, average='macro', zero_division=0)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"SENTIMENT CLASSIFICATION RESULTS\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"Accuracy:  {sent_accuracy:.4f}\")\n",
        "print(f\"Precision: {sent_precision:.4f} (macro)\")\n",
        "print(f\"Recall:    {sent_recall:.4f} (macro)\")\n",
        "print(f\"F1-Score:  {sent_f1:.4f} (macro)\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true_sent, y_pred_sent))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_sent = confusion_matrix(y_true_sent, y_pred_sent)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate predictions sequentially\n",
        "def generate_topic_predictions_sequential():    \n",
        "    topic_predictions = []\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Generating Topic Predictions\"):\n",
        "        prompt = create_topic_prompt(row['question'])\n",
        "        pred = query_llm(prompt, MODEL_NAME)\n",
        "        topic_predictions.append(pred)\n",
        "    print(f\"\\n✓ Topic predictions completed\")\n",
        "    return topic_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from typing import Any\n",
        "\n",
        "def process_topic_chunk(chunk, model, api_key, chunk_id):\n",
        "    client = Groq(api_key=api_key)\n",
        "\n",
        "    topic_predictions = []\n",
        "\n",
        "    try:\n",
        "        for idx, text in tqdm(enumerate(chunk), total=len(chunk), desc=f\"Generating Sentiment Predictions for chunk_id {chunk_id}\"):\n",
        "            prompt = create_topic_prompt(text)\n",
        "            response = client.chat.completions.create(\n",
        "                model=model,\n",
        "                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                temperature=0.0,  # Deterministic for consistency\n",
        "                max_tokens=500,\n",
        "                top_p=1,\n",
        "            )\n",
        "            topic_predictions.append(response.choices[0].message.content.strip())\n",
        "            time.sleep(0.05)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in Topic processing of chunk_id {chunk_id}: {e}\")\n",
        "        topic_predictions.append(\"No topic found\")\n",
        "\n",
        "    return topic_predictions\n",
        "\n",
        "def generate_topic_predictions_parallel(n_workers, inputs):\n",
        "    # Estimate processing time\n",
        "    estimated_time_sequential = len(inputs) * 5 / 60  # 5 seconds per text\n",
        "    estimated_time_parallel = estimated_time_sequential / n_workers \n",
        "    print(f\"Estimated time:\")\n",
        "    print(f\" Sequential: ~{estimated_time_sequential:.1f} minutes\")\n",
        "    print(f\" Parallel ({n_workers} workers): ~{estimated_time_parallel:.1f} minutes\")\n",
        "    print(f\" Speedup: ~{estimated_time_sequential/estimated_time_parallel:.1f}x faster!\")\n",
        "\n",
        "    print(f\"Using {n_workers} workers for parallel topic detection...\")\n",
        "    print(f\"Processing {len(inputs)} texts in parallel batches...\")\n",
        "\n",
        "    # Split texts into chunks for each worker\n",
        "    chunk_size = max(1, len(inputs) // n_workers)\n",
        "    text_chunks = []\n",
        "\n",
        "    for i in range(0, len(inputs), chunk_size):\n",
        "        text_chunks.append(inputs[i:i+chunk_size])\n",
        "\n",
        "    topic_predictions = []\n",
        "\n",
        "    try:\n",
        "        with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
        "            # Submit all chunks to workers\n",
        "            future_to_chunk = {\n",
        "                executor.submit(process_topic_chunk, chunk, MODEL_NAME, LLM_API_KEY, i): (chunk, i)\n",
        "                for i, chunk in enumerate(text_chunks)\n",
        "            }\n",
        "\n",
        "            chunk_results = {}\n",
        "            with tqdm(total=len(inputs), desc=\"Processing Topic Predictions\") as pbar:\n",
        "                for future in as_completed(future_to_chunk):\n",
        "                    try:\n",
        "                        chunk_data,chunk_id = future_to_chunk[future]\n",
        "                        results = future.result()\n",
        "                        chunk_results[chunk_id] = results\n",
        "                        pbar.update(len(results))\n",
        "                        print(f\"Processed chunk_id {chunk_id} with {len(results)} predictions\")\n",
        "                    except Exception as e:\n",
        "                        chunk_data, chunk_id = future_to_chunk[future]\n",
        "                        error_results = [\"No topic found\"] * len(chunk_data)\n",
        "                        chunk_results[chunk_id] = error_results\n",
        "                        print(f\"Error in Topic processing of chunk_id {chunk_id}: {e}\")\n",
        "            for chunk_id in sorted(chunk_results.keys()):\n",
        "                topic_predictions.extend(chunk_results[chunk_id])\n",
        "    except Exception as e:\n",
        "        print(f\"Error in topic processing: {e}\")\n",
        "    return topic_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 8. TASK 3: TOPIC DETECTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TASK 3: TOPIC DETECTION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Generate predictions\n",
        "print(\"\\n⏳ Generating Topic Predictions...\")\n",
        "\n",
        "cpu_count = mp.cpu_count()\n",
        "\n",
        "if cpu_count > 4:\n",
        "    n_workers = min(mp.cpu_count(), 8)  # Cap at 8 workers for API rate limits\n",
        "else:\n",
        "    n_workers = cpu_count\n",
        "\n",
        "inputs = df['question'].tolist()\n",
        "\n",
        "topic_preds = []\n",
        "if n_workers < 4 or len(inputs) < 100:\n",
        "    print(\"Using sequential generation for Topic Detection\")\n",
        "    topic_preds = generate_topic_predictions_sequential()\n",
        "else:\n",
        "    print(\"Using parallel generation for Topic Detection\")\n",
        "    topic_preds = generate_topic_predictions_parallel(n_workers,inputs)\n",
        "\n",
        "df['topic_pred'] = topic_preds\n",
        "print(f\"\\n✓ Topic predictions completed\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kcCt4IbOxwbf",
        "outputId": "87db714a-f51e-4a26-fe49-4c9fe83c400f"
      },
      "outputs": [],
      "source": [
        "# Evaluate Topic Detection\n",
        "y_true_topic = df['topic_true']\n",
        "y_pred_topic = df['topic_pred']\n",
        "\n",
        "topic_accuracy = accuracy_score(y_true_topic, y_pred_topic)\n",
        "topic_macro_f1 = f1_score(y_true_topic, y_pred_topic, average='macro', zero_division=0)\n",
        "topic_micro_f1 = f1_score(y_true_topic, y_pred_topic, average='micro', zero_division=0)\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"TOPIC DETECTION RESULTS\")\n",
        "print(\"-\" * 80)\n",
        "print(f\"Accuracy:   {topic_accuracy:.4f}\")\n",
        "print(f\"Macro-F1:   {topic_macro_f1:.4f}\")\n",
        "print(f\"Micro-F1:   {topic_micro_f1:.4f}\")\n",
        "\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_true_topic, y_pred_topic))\n",
        "\n",
        "# Confusion Matrix\n",
        "cm_topic = confusion_matrix(y_true_topic, y_pred_topic,\n",
        "                            labels=['Revenue', 'Risk', 'Operations', 'Legal', 'Management', 'Other'])\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(cm_topic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxaMGPu01yuO",
        "outputId": "8d0bee34-b48f-4c84-e55e-ff52399273e0"
      },
      "outputs": [],
      "source": [
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"CONSOLIDATED RESULTS SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "results_summary = pd.DataFrame({\n",
        "    'Task': ['Question Answering', 'Question Answering', 'Sentiment Classification',\n",
        "             'Sentiment Classification', 'Sentiment Classification', 'Sentiment Classification',\n",
        "             'Topic Detection', 'Topic Detection', 'Topic Detection'],\n",
        "    'Metric': ['Exact Match', 'Token F1', 'Accuracy', 'Precision (Macro)', 'Recall (Macro)',\n",
        "               'F1 (Macro)', 'Accuracy', 'Macro-F1', 'Micro-F1'],\n",
        "    'Score': [\n",
        "        np.mean(qa_em_scores),\n",
        "        np.mean(qa_f1_scores),\n",
        "        sent_accuracy,\n",
        "        sent_precision,\n",
        "        sent_recall,\n",
        "        sent_f1,\n",
        "        topic_accuracy,\n",
        "        topic_macro_f1,\n",
        "        topic_micro_f1\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"\\n\", results_summary.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "uG6x97QG1vy9",
        "outputId": "7e29a0bb-4c7f-42b7-abc8-73195373c161"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 10. VISUALIZATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"GENERATING VISUALIZATIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# Plot 1: QA Performance\n",
        "ax1 = axes[0, 0]\n",
        "qa_metrics = ['Exact Match', 'Token F1']\n",
        "qa_scores = [np.mean(qa_em_scores), np.mean(qa_f1_scores)]\n",
        "ax1.bar(qa_metrics, qa_scores, color=['#3498db', '#2ecc71'])\n",
        "ax1.set_ylim(0, 1)\n",
        "ax1.set_ylabel('Score')\n",
        "ax1.set_title('Question Answering Performance')\n",
        "ax1.grid(axis='y', alpha=0.3)\n",
        "for i, v in enumerate(qa_scores):\n",
        "    ax1.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "# Plot 2: Sentiment Classification\n",
        "ax2 = axes[0, 1]\n",
        "sent_metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
        "sent_scores = [sent_accuracy, sent_precision, sent_recall, sent_f1]\n",
        "ax2.bar(sent_metrics, sent_scores, color=['#e74c3c', '#9b59b6', '#f39c12', '#1abc9c'])\n",
        "ax2.set_ylim(0, 1)\n",
        "ax2.set_ylabel('Score')\n",
        "ax2.set_title('Sentiment Classification Performance')\n",
        "ax2.grid(axis='y', alpha=0.3)\n",
        "for i, v in enumerate(sent_scores):\n",
        "    ax2.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "# Plot 3: Topic Detection\n",
        "ax3 = axes[1, 0]\n",
        "topic_metrics = ['Accuracy', 'Macro-F1', 'Micro-F1']\n",
        "topic_scores = [topic_accuracy, topic_macro_f1, topic_micro_f1]\n",
        "ax3.bar(topic_metrics, topic_scores, color=['#34495e', '#16a085', '#27ae60'])\n",
        "ax3.set_ylim(0, 1)\n",
        "ax3.set_ylabel('Score')\n",
        "ax3.set_title('Topic Detection Performance')\n",
        "ax3.grid(axis='y', alpha=0.3)\n",
        "for i, v in enumerate(topic_scores):\n",
        "    ax3.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "# Plot 4: Task Comparison\n",
        "ax4 = axes[1, 1]\n",
        "tasks = ['QA\\n(F1)', 'Sentiment\\n(F1)', 'Topic\\n(Macro-F1)']\n",
        "comparison_scores = [np.mean(qa_f1_scores), sent_f1, topic_macro_f1]\n",
        "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
        "ax4.bar(tasks, comparison_scores, color=colors)\n",
        "ax4.set_ylim(0, 1)\n",
        "ax4.set_ylabel('F1 Score')\n",
        "ax4.set_title('Cross-Task Performance Comparison')\n",
        "ax4.grid(axis='y', alpha=0.3)\n",
        "for i, v in enumerate(comparison_scores):\n",
        "    ax4.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('financial_analysis_results.png', dpi=300, bbox_inches='tight')\n",
        "print(\"✓ Visualizations saved as 'financial_analysis_results.png'\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9TliJmkg1s3w",
        "outputId": "3f550974-6320-4991-d44c-2caa7acb5e61"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 11. SAVE RESULTS\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"SAVING RESULTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Save detailed predictions\n",
        "output_df = df[['question', 'answer', 'context',\n",
        "                'qa_pred', 'qa_em', 'qa_f1',\n",
        "                'sentiment_true', 'sentiment_pred',\n",
        "                'topic_true', 'topic_pred']]\n",
        "output_df.to_csv('financial_analysis_predictions.csv', index=False)\n",
        "print(\"✓ Detailed predictions saved to 'financial_analysis_predictions.csv'\")\n",
        "\n",
        "# Save summary results\n",
        "results_summary.to_csv('financial_analysis_summary.csv', index=False)\n",
        "print(\"✓ Summary results saved to 'financial_analysis_summary.csv'\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"PIPELINE COMPLETED SUCCESSFULLY\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nModel: {GROQ_70B}\")\n",
        "print(f\"Prompting Strategy: Zero-Shot\")\n",
        "print(f\"Total Records Processed: {len(df)}\")\n",
        "print(f\"\\nKey Findings:\")\n",
        "print(f\"  • QA Token F1: {np.mean(qa_f1_scores):.3f}\")\n",
        "print(f\"  • Sentiment F1: {sent_f1:.3f}\")\n",
        "print(f\"  • Topic Macro-F1: {topic_macro_f1:.3f}\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hz6Xqkyf2B6S",
        "outputId": "0d6fe1f0-7029-403b-d6d5-51b64c098016"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 12. EXPLAINABLE AI (XAI) - SENTIMENT CLASSIFICATION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EXPLAINABLE AI ANALYSIS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ============================================================================\n",
        "# 12.1 PREPARE SURROGATE MODEL FOR SENTIMENT\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[1/4] Training surrogate model for Sentiment Classification...\")\n",
        "\n",
        "# Create TF-IDF features\n",
        "tfidf_sent = TfidfVectorizer(max_features=100, ngram_range=(1, 2), dtype=float)\n",
        "# X_sent = tfidf_sent.fit_transform(df['context'])\n",
        "X_sent = tfidf_sent.fit_transform(df['combined_text'])\n",
        "\n",
        "# Train a surrogate Random Forest model\n",
        "rf_sent = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
        "rf_sent.fit(X_sent, df['sentiment_true'])\n",
        "\n",
        "print(f\"✓ Surrogate model trained (Accuracy: {rf_sent.score(X_sent, df['sentiment_true']):.3f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 791
        },
        "id": "siXZYXZYoLg9",
        "outputId": "1cc97784-993a-4645-fe95-6c4161afe603"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 12.2 SHAP ANALYSIS FOR SENTIMENT\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[2/4] Generating SHAP explanations for Sentiment...\")\n",
        "\n",
        "X_sent_dense = X_sent.toarray()\n",
        "\n",
        "# Create SHAP explainer\n",
        "explainer_sent = shap.TreeExplainer(rf_sent)\n",
        "shap_values_sent = explainer_sent.shap_values(X_sent_dense)\n",
        "\n",
        "# SHAP Summary Plot\n",
        "fig_shap_sent, axes_shap = plt.subplots(1, 2, figsize=(18, 6))\n",
        "\n",
        "# Plot for each sentiment class (assuming 3 classes: Negative=0, Neutral=1, Positive=2)\n",
        "class_names_sent = ['Negative', 'Neutral', 'Positive']\n",
        "feature_names_sent = tfidf_sent.get_feature_names_out()\n",
        "\n",
        "shap_values_class_2 = shap_values_sent[:, :, 2]\n",
        "# Summary plot for Positive class\n",
        "if len(shap_values_sent) >= 3:\n",
        "    plt.sca(axes_shap[0])\n",
        "    shap_values_class_2 = shap_values_sent[:, :, 2]\n",
        "    shap.summary_plot(shap_values_class_2, X_sent_dense,\n",
        "                     feature_names=feature_names_sent,\n",
        "                     plot_type=\"bar\", show=False, max_display=15)\n",
        "    axes_shap[0].set_title('SHAP Feature Importance - Positive Sentiment', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Summary plot for Negative class\n",
        "    plt.sca(axes_shap[1])\n",
        "    shap_values_negative = shap_values_sent[:, :, 0]\n",
        "    shap.summary_plot(shap_values_negative, X_sent,\n",
        "                     feature_names=feature_names_sent,\n",
        "                     plot_type=\"bar\", show=False, max_display=15)\n",
        "    axes_shap[1].set_title('SHAP Feature Importance - Negative Sentiment', fontsize=14, fontweight='bold')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fmCDMYWv2LMX",
        "outputId": "d9d4cb62-3433-453b-c2d6-6277bc82849b"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 12.2 SHAP ANALYSIS FOR SENTIMENT\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[2/4] Generating SHAP explanations for Sentiment...\")\n",
        "\n",
        "X_sent_dense = X_sent.toarray()\n",
        "\n",
        "# Create SHAP explainer\n",
        "explainer_sent = shap.TreeExplainer(rf_sent)\n",
        "shap_values_sent = explainer_sent.shap_values(X_sent_dense)\n",
        "\n",
        "# SHAP Summary Plot\n",
        "fig_shap_sent, axes_shap = plt.subplots(1, 2, figsize=(18, 6))\n",
        "\n",
        "# Plot for each sentiment class (assuming 3 classes: Negative=0, Neutral=1, Positive=2)\n",
        "class_names_sent = ['Negative', 'Neutral', 'Positive']\n",
        "feature_names_sent = tfidf_sent.get_feature_names_out()\n",
        "\n",
        "shap_values_class_2 = shap_values_sent[:, :, 2]\n",
        "# Summary plot for Positive class\n",
        "if len(shap_values_sent) >= 3:\n",
        "    plt.sca(axes_shap[0])\n",
        "    shap_values_class_2 = shap_values_sent[:, :, 2]\n",
        "    shap.summary_plot(shap_values_class_2, X_sent_dense,\n",
        "                     feature_names=feature_names_sent,\n",
        "                     plot_type=\"bar\", show=False, max_display=15)\n",
        "    axes_shap[0].set_title('SHAP Feature Importance - Positive Sentiment', fontsize=14, fontweight='bold')\n",
        "\n",
        "    # Summary plot for Negative class\n",
        "    plt.sca(axes_shap[1])\n",
        "    shap_values_negative = shap_values_sent[:, :, 0]\n",
        "    shap.summary_plot(shap_values_negative, X_sent,\n",
        "                     feature_names=feature_names_sent,\n",
        "                     plot_type=\"bar\", show=False, max_display=15)\n",
        "    axes_shap[1].set_title('SHAP Feature Importance - Negative Sentiment', fontsize=14, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('xai_shap_sentiment.png', dpi=300, bbox_inches='tight')\n",
        "print(\"✓ SHAP plots saved as 'xai_shap_sentiment.png'\")\n",
        "plt.show()\n",
        "\n",
        "# SHAP Force Plot for a specific example\n",
        "print(\"\\nGenerating SHAP force plot for sample prediction...\")\n",
        "sample_idx = 0\n",
        "shap.initjs()\n",
        "\n",
        "# Individual SHAP explanation\n",
        "if len(shap_values_sent) >= 3:\n",
        "    fig_force = plt.figure(figsize=(14, 3))\n",
        "    shap_explanation = shap.Explanation(\n",
        "        values=shap_values_sent[2][sample_idx],\n",
        "        base_values=explainer_sent.expected_value[2],\n",
        "        data=X_sent[sample_idx].toarray()[0],\n",
        "        feature_names=feature_names_sent\n",
        "    )\n",
        "\n",
        "    # Waterfall plot\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    shap.plots.waterfall(shap_explanation, max_display=15, show=False)\n",
        "    plt.title(f'SHAP Waterfall Plot - Sample {sample_idx} (Positive Class)', fontsize=14, fontweight='bold')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('xai_shap_waterfall_sentiment.png', dpi=300, bbox_inches='tight')\n",
        "    print(\"✓ SHAP waterfall plot saved as 'xai_shap_waterfall_sentiment.png'\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "id": "_62QjDM52Ibe",
        "outputId": "d8e9e69f-7905-4ce6-8ea8-32bcb21b8a88"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 12.3 LIME ANALYSIS FOR SENTIMENT\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[3/4] Generating LIME explanations for Sentiment...\")\n",
        "\n",
        "# Create LIME explainer\n",
        "def sentiment_predictor(texts):\n",
        "    \"\"\"Prediction function for LIME\"\"\"\n",
        "    X = tfidf_sent.transform(texts)\n",
        "    return rf_sent.predict_proba(X)\n",
        "\n",
        "lime_sent = LimeTextExplainer(class_names=class_names_sent)\n",
        "\n",
        "# Generate LIME explanation for sample instances\n",
        "lime_explanations_sent = []\n",
        "fig_lime_sent, axes_lime = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes_lime = axes_lime.flatten()\n",
        "\n",
        "for idx in range(min(4, len(df))):\n",
        "    exp = lime_sent.explain_instance(\n",
        "        df.iloc[idx]['context'],\n",
        "        sentiment_predictor,\n",
        "        num_features=10,\n",
        "        top_labels=1\n",
        "    )\n",
        "    lime_explanations_sent.append(exp)\n",
        "\n",
        "    # Plot explanation\n",
        "    fig_temp = exp.as_pyplot_figure(label=exp.top_labels[0])\n",
        "\n",
        "    # Transfer to subplot\n",
        "    ax = axes_lime[idx]\n",
        "    ax.clear()\n",
        "\n",
        "    # Get feature importance\n",
        "    feat_imp = exp.as_list(label=exp.top_labels[0])\n",
        "    features = [f[0] for f in feat_imp]\n",
        "    weights = [f[1] for f in feat_imp]\n",
        "    colors = ['green' if w > 0 else 'red' for w in weights]\n",
        "\n",
        "    y_pos = np.arange(len(features))\n",
        "    ax.barh(y_pos, weights, color=colors, alpha=0.7)\n",
        "    ax.set_yticks(y_pos)\n",
        "    ax.set_yticklabels(features, fontsize=9)\n",
        "    ax.set_xlabel('Feature Weight', fontsize=10)\n",
        "    ax.set_title(f'LIME Explanation - Sample {idx}\\nPredicted: {class_names_sent[exp.top_labels[0]]}',\n",
        "                 fontsize=11, fontweight='bold')\n",
        "    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
        "    ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "    plt.close(fig_temp)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('xai_lime_sentiment.png', dpi=300, bbox_inches='tight')\n",
        "print(\"✓ LIME plots saved as 'xai_lime_sentiment.png'\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NmSYnsB_2GE-",
        "outputId": "7d89d9c0-3df7-4aaa-895b-b4f8fdc4cb39"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 12.4 PREPARE SURROGATE MODEL FOR TOPIC DETECTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n[4/4] Training surrogate model for Topic Detection...\")\n",
        "\n",
        "# Create TF-IDF features for topics\n",
        "tfidf_topic = TfidfVectorizer(max_features=100, ngram_range=(1, 2))\n",
        "X_topic = tfidf_topic.fit_transform(df['question'])\n",
        "\n",
        "# Train a surrogate Random Forest model\n",
        "rf_topic = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
        "rf_topic.fit(X_topic, df['topic_true'])\n",
        "\n",
        "print(f\"✓ Surrogate model trained (Accuracy: {rf_topic.score(X_topic, df['topic_true']):.3f})\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 688
        },
        "id": "0hsLMu3O2EVi",
        "outputId": "3d31d754-7869-4e27-83b1-3567604be97b"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 12.5 SHAP ANALYSIS FOR TOPIC DETECTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nGenerating SHAP explanations for Topic Detection...\")\n",
        "\n",
        "X_topic_dense = X_topic.toarray()\n",
        "\n",
        "# Create SHAP explainer\n",
        "explainer_topic = shap.TreeExplainer(rf_topic)\n",
        "shap_values_topic = explainer_topic.shap_values(X_topic_dense)\n",
        "\n",
        "# SHAP Summary Plot for Topics\n",
        "fig_shap_topic, axes_shap_topic = plt.subplots(2, 2, figsize=(18, 12))\n",
        "axes_shap_topic = axes_shap_topic.flatten()\n",
        "\n",
        "# Get unique topics and their indices\n",
        "unique_topics = rf_topic.classes_\n",
        "feature_names_topic = tfidf_topic.get_feature_names_out()\n",
        "\n",
        "for i, topic in enumerate(unique_topics[:4]):  # Show first 4 topics\n",
        "    if i < len(shap_values_topic):\n",
        "        plt.sca(axes_shap_topic[i])\n",
        "        shap_values_class_i = shap_values_topic[:, :, i]\n",
        "        shap.summary_plot(shap_values_class_i, X_topic,\n",
        "                         feature_names=feature_names_topic,\n",
        "                         plot_type=\"bar\", show=False, max_display=12)\n",
        "        axes_shap_topic[i].set_title(f'SHAP Feature Importance - {topic}',\n",
        "                                     fontsize=13, fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('xai_shap_topic.png', dpi=300, bbox_inches='tight')\n",
        "print(\"✓ SHAP topic plots saved as 'xai_shap_topic.png'\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "id": "djoqvSL_2aTm",
        "outputId": "1e536095-cf5b-4945-918d-39c226b60b8b"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 12.6 LIME ANALYSIS FOR TOPIC DETECTION\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nGenerating LIME explanations for Topic Detection...\")\n",
        "\n",
        "# Create LIME explainer for topics\n",
        "def topic_predictor(texts):\n",
        "    \"\"\"Prediction function for LIME\"\"\"\n",
        "    X = tfidf_topic.transform(texts)\n",
        "    return rf_topic.predict_proba(X)\n",
        "\n",
        "lime_topic = LimeTextExplainer(class_names=unique_topics.tolist())\n",
        "\n",
        "# Generate LIME explanation for sample instances\n",
        "fig_lime_topic, axes_lime_topic = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes_lime_topic = axes_lime_topic.flatten()\n",
        "\n",
        "for idx in range(min(4, len(df))):\n",
        "    exp = lime_topic.explain_instance(\n",
        "        df.iloc[idx]['question'],\n",
        "        topic_predictor,\n",
        "        num_features=8,\n",
        "        top_labels=1\n",
        "    )\n",
        "\n",
        "    # Plot explanation\n",
        "    ax = axes_lime_topic[idx]\n",
        "    ax.clear()\n",
        "\n",
        "    # Get feature importance\n",
        "    feat_imp = exp.as_list(label=exp.top_labels[0])\n",
        "    features = [f[0] for f in feat_imp]\n",
        "    weights = [f[1] for f in feat_imp]\n",
        "    colors = ['green' if w > 0 else 'red' for w in weights]\n",
        "\n",
        "    y_pos = np.arange(len(features))\n",
        "    ax.barh(y_pos, weights, color=colors, alpha=0.7)\n",
        "    ax.set_yticks(y_pos)\n",
        "    ax.set_yticklabels(features, fontsize=9)\n",
        "    ax.set_xlabel('Feature Weight', fontsize=10)\n",
        "    pred_topic = unique_topics[exp.top_labels[0]]\n",
        "    ax.set_title(f'LIME Explanation - Sample {idx}\\nPredicted Topic: {pred_topic}',\n",
        "                 fontsize=11, fontweight='bold')\n",
        "    ax.axvline(x=0, color='black', linestyle='-', linewidth=0.8)\n",
        "    ax.grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('xai_lime_topic.png', dpi=300, bbox_inches='tight')\n",
        "print(\"✓ LIME topic plots saved as 'xai_lime_topic.png'\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 517
        },
        "id": "DFJfeZq82is3",
        "outputId": "4574eccc-3e67-47e0-bc58-e6254769af10"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 12.7 FEATURE IMPORTANCE COMPARISON\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nGenerating Feature Importance Comparison...\")\n",
        "\n",
        "fig_importance, axes_imp = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "# Sentiment Feature Importance\n",
        "feature_importance_sent = rf_sent.feature_importances_\n",
        "top_n = 15\n",
        "indices_sent = np.argsort(feature_importance_sent)[-top_n:]\n",
        "top_features_sent = [feature_names_sent[i] for i in indices_sent]\n",
        "top_importance_sent = feature_importance_sent[indices_sent]\n",
        "\n",
        "axes_imp[0].barh(range(top_n), top_importance_sent, color='#3498db', alpha=0.8)\n",
        "axes_imp[0].set_yticks(range(top_n))\n",
        "axes_imp[0].set_yticklabels(top_features_sent, fontsize=9)\n",
        "axes_imp[0].set_xlabel('Feature Importance', fontsize=11)\n",
        "axes_imp[0].set_title('Top 15 Features - Sentiment Classification\\n(Random Forest)',\n",
        "                      fontsize=13, fontweight='bold')\n",
        "axes_imp[0].grid(axis='x', alpha=0.3)\n",
        "\n",
        "# Topic Feature Importance\n",
        "feature_importance_topic = rf_topic.feature_importances_\n",
        "indices_topic = np.argsort(feature_importance_topic)[-top_n:]\n",
        "top_features_topic = [feature_names_topic[i] for i in indices_topic]\n",
        "top_importance_topic = feature_importance_topic[indices_topic]\n",
        "\n",
        "axes_imp[1].barh(range(top_n), top_importance_topic, color='#2ecc71', alpha=0.8)\n",
        "axes_imp[1].set_yticks(range(top_n))\n",
        "axes_imp[1].set_yticklabels(top_features_topic, fontsize=9)\n",
        "axes_imp[1].set_xlabel('Feature Importance', fontsize=11)\n",
        "axes_imp[1].set_title('Top 15 Features - Topic Detection\\n(Random Forest)',\n",
        "                      fontsize=13, fontweight='bold')\n",
        "axes_imp[1].grid(axis='x', alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('xai_feature_importance.png', dpi=300, bbox_inches='tight')\n",
        "print(\"✓ Feature importance plots saved as 'xai_feature_importance.png'\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frXWjkrV2gd_",
        "outputId": "90c60714-b917-4a0f-e5ac-21d92f82a520"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 12.8 XAI METRICS AND SUMMARY\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"XAI ANALYSIS SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calculate explainability metrics\n",
        "xai_metrics = {\n",
        "    'Task': [],\n",
        "    'Surrogate Model Accuracy': [],\n",
        "    'Top Feature': [],\n",
        "    'Feature Importance': []\n",
        "}\n",
        "\n",
        "# Sentiment XAI metrics\n",
        "xai_metrics['Task'].append('Sentiment Classification')\n",
        "xai_metrics['Surrogate Model Accuracy'].append(f\"{rf_sent.score(X_sent, df['sentiment_true']):.4f}\")\n",
        "top_sent_idx = np.argmax(feature_importance_sent)\n",
        "xai_metrics['Top Feature'].append(feature_names_sent[top_sent_idx])\n",
        "xai_metrics['Feature Importance'].append(f\"{feature_importance_sent[top_sent_idx]:.4f}\")\n",
        "\n",
        "# Topic XAI metrics\n",
        "xai_metrics['Task'].append('Topic Detection')\n",
        "xai_metrics['Surrogate Model Accuracy'].append(f\"{rf_topic.score(X_topic, df['topic_true']):.4f}\")\n",
        "top_topic_idx = np.argmax(feature_importance_topic)\n",
        "xai_metrics['Top Feature'].append(feature_names_topic[top_topic_idx])\n",
        "xai_metrics['Feature Importance'].append(f\"{feature_importance_topic[top_topic_idx]:.4f}\")\n",
        "\n",
        "xai_summary_df = pd.DataFrame(xai_metrics)\n",
        "print(\"\\n\", xai_summary_df.to_string(index=False))\n",
        "\n",
        "# Save XAI summary\n",
        "xai_summary_df.to_csv('xai_analysis_summary.csv', index=False)\n",
        "print(\"\\n✓ XAI summary saved to 'xai_analysis_summary.csv'\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LvF7OpFc2cwE",
        "outputId": "205f6ff3-2f6e-43ee-fb26-1cbe7f3f9cb8"
      },
      "outputs": [],
      "source": [
        "# ============================================================================\n",
        "# 12.9 DETAILED XAI EXPLANATIONS FOR KEY INSTANCES\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n\" + \"-\" * 80)\n",
        "print(\"DETAILED XAI EXPLANATIONS FOR KEY INSTANCES\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Select one instance from each task for detailed explanation\n",
        "sample_idx_detailed = 0\n",
        "\n",
        "print(f\"\\n📋 Sample {sample_idx_detailed} - Detailed Analysis\")\n",
        "print(\"-\" * 80)\n",
        "\n",
        "# Original data\n",
        "print(f\"\\nOriginal Context: {df.iloc[sample_idx_detailed]['context'][:200]}...\")\n",
        "print(f\"True Sentiment: {df.iloc[sample_idx_detailed]['sentiment_true']}\")\n",
        "print(f\"Predicted Sentiment: {df.iloc[sample_idx_detailed]['sentiment_pred']}\")\n",
        "\n",
        "# SHAP values for this instance\n",
        "if len(shap_values_sent) >= 3:\n",
        "    print(f\"\\nTop 5 SHAP Features (Sentiment):\")\n",
        "    shap_instance = shap_values_sent[2][sample_idx_detailed]  # Positive class\n",
        "    top_shap_indices = np.argsort(np.abs(shap_instance))[-5:][::-1]\n",
        "    for i, idx in enumerate(top_shap_indices, 1):\n",
        "        print(f\"  {i}. {feature_names_sent[idx]}: {shap_instance[idx]:.4f}\")\n",
        "\n",
        "# LIME explanation\n",
        "print(f\"\\nLIME Explanation (Sentiment):\")\n",
        "lime_exp_detailed = lime_explanations_sent[sample_idx_detailed]\n",
        "for feat, weight in lime_exp_detailed.as_list(label=lime_exp_detailed.top_labels[0])[:5]:\n",
        "    print(f\"  '{feat}': {weight:.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"XAI ANALYSIS COMPLETED\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\n✓ Generated Visualizations:\")\n",
        "print(\"  1. xai_shap_sentiment.png - SHAP feature importance for sentiment\")\n",
        "print(\"  2. xai_shap_waterfall_sentiment.png - SHAP waterfall plot\")\n",
        "print(\"  3. xai_lime_sentiment.png - LIME explanations for sentiment\")\n",
        "print(\"  4. xai_shap_topic.png - SHAP feature importance for topics\")\n",
        "print(\"  5. xai_lime_topic.png - LIME explanations for topics\")\n",
        "print(\"  6. xai_feature_importance.png - Feature importance comparison\")\n",
        "print(\"\\n✓ Saved Reports:\")\n",
        "print(\"  1. xai_analysis_summary.csv - XAI metrics summary\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"COMPLETE PIPELINE FINISHED\")\n",
        "print(\"=\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rjto-AOtrpfT",
        "outputId": "8f2f14ac-b1ce-4c01-b8f5-45e4b00ed306"
      },
      "outputs": [],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r_XNdR8Fsma8"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
